# -*- coding: utf-8 -*-
"""Cleaning_Data.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/124yzWZ_uKR2ihx9vVALkuTijnDvy3D8v

### Set Up
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt  
import seaborn as seabornInstance 
from sklearn.model_selection import train_test_split 
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures
from sklearn.metrics import mean_squared_error, r2_score
from sklearn import metrics 
from sklearn import datasets
from sklearn.multiclass import OneVsRestClassifier
from sklearn.svm import LinearSVC
from sklearn.datasets import fetch_mldata
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn import tree
from sklearn.neural_network import MLPClassifier
from sklearn.preprocessing import label_binarize
from sklearn.neural_network import MLPRegressor
from sklearn.metrics import roc_auc_score
import seaborn as sn
import os

from google.colab import files
uploaded = files.upload()

df = pd.read_csv('./steam_raw.csv')

df.head()

"""### Data Cleaning"""

# remove unwanted columns
to_drop = ['appid', 'name', 'english', 'developer', 'publisher', 'platforms', 'required_age', 'genres', 'steamspy_tags']
df.drop(to_drop, inplace=True, axis=1)

# remove zeros in columns
df['price'] = df['price'].astype(float)
df['achievements'] = df['achievements'].astype(float)
df['average_playtime'] = df['average_playtime'].astype(float)
df = df[df.price != 0]
#df = df[df.achievements != 0]
#df = df[df.average_playtime != 0]
#df = df[df.median_playtime != 0]
df.describe()

# combine ratings
df.insert(1, 'total_ratings', ' ', True)
df.total_ratings = df.positive_ratings+df.negative_ratings;

df.insert(4, 'ratings', ' ', True) 
df['ratings'] = df['positive_ratings']/(df['positive_ratings'] + df['negative_ratings'])

# remove unwanted ratings columns
to_drop = ['positive_ratings', 'negative_ratings']
df.drop(to_drop, inplace=True, axis=1)
df.head()

# extract the year
df['release_year'] = pd.DatetimeIndex(df['release_date']).year

# remove release_date column
to_drop = ['release_date']
df.drop(to_drop, inplace=True, axis=1)

# split owners
df[['min_owners','max_owners']] = df['owners'].str.split('-',expand=True)

# calculate average yearly revenue
df.insert(8, 'avg_revenue', ' ', True) 
df['min_owners'] = df['min_owners'].astype(float)
df['max_owners'] = df['max_owners'].astype(float)
df['price'] = df['price'].astype(float)
df['release_year'] = df['release_year'].astype(float)
df['avg_revenue'] = (df['min_owners'] + df['max_owners'])/2 * df['price'] / (2020 - df['release_year'])

# remove columns
to_drop = ['release_year', 'min_owners', 'max_owners', 'owners']
df.drop(to_drop, inplace=True, axis=1)

# insert column for multi-player and single-player
df.insert(1, 'singleplayer', ' ', True) 
df.insert(2, 'multiplayer', ' ', True) 

# make categories lowercase
df['categories'] = df['categories'].str.lower()

# Populate columns with True/False
df['singleplayer'] = df['categories'].str.contains('single-player')
df['multiplayer'] = df['categories'].str.contains('multi-player')

# Change to binary 0/1
df['singleplayer'] = df['singleplayer'].astype(int)
df['multiplayer'] = df['multiplayer'].astype(int)

to_drop = ['categories']
df.drop(to_drop, inplace=True, axis=1)

# mean normalization + feature scaling
achievements_std = df['achievements'].std()
achievements_mean = df.achievements.mean()
df.achievements = (df.achievements - achievements_mean)/achievements_std

price_std = df['price'].std()
price_mean = df.price.mean()
df.price = (df.price - price_mean)/price_std

avg_playtime_std = df['average_playtime'].std()
avg_playtime_mean = df.average_playtime.mean()
df.average_playtime = (df.average_playtime - avg_playtime_mean)/avg_playtime_std

med_playtime_std = df['median_playtime'].std()
med_playtime_mean = df.median_playtime.mean()
df.median_playtime = (df.median_playtime - med_playtime_mean)/med_playtime_std

total_ratings_std = df.total_ratings.std()
total_ratings_mean = df.total_ratings.mean()
df.total_ratings = (df.total_ratings - total_ratings_mean)/total_ratings_std

#df.avg_revenue = np.log(df.avg_revenue)
#df.to_csv('steam_v1.csv')
#files.download('steam_v1.csv')

df.describe()

# dropping average_playtime and median_playtime
to_drop = ['average_playtime', 'median_playtime']
df.drop(to_drop, inplace=True, axis=1)
df.head()

df.shape

"""### Binning data"""

# Run this if rerunning binning

#df.hist(column='avg_revenue', bins ='auto')
to_drop = ['bin']
df.drop(to_drop, inplace=True, axis=1)

# Split into bins
df.insert(7, 'bins', ' ', True) 
bin_num = np.linspace(start=0,stop=3,num=4)
df.bins = pd.qcut(df.avg_revenue, q=4, labels=bin_num)

#bins_revenue = [0, 10000, 50000, 200000, 1000000, 20000000, 700000000]
#df.bins = pd.cut(df.avg_revenue, bins=bins_revenue, labels=bin_num)
df['bins'] = df['bins'].astype(int)
df.head()

"""### Split data into training and test"""

# split data
X = df[['singleplayer', 'multiplayer', 'ratings', 'total_ratings', 'achievements', 'price']].values
y = df['bins'].values

X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.4, shuffle='true')
X_val, X_test, y_val, y_test = train_test_split(X_val, y_val, test_size=0.5, shuffle='true')

"""### Confusion Matrix Code"""

def plot_confusion_matrix(cm,
                          target_names,
                          title='Confusion matrix',
                          cmap=None,
                          normalize=True):
    """
    given a sklearn confusion matrix (cm), make a nice plot

    Arguments
    ---------
    cm:           confusion matrix from sklearn.metrics.confusion_matrix

    target_names: given classification classes such as [0, 1, 2]
                  the class names, for example: ['high', 'medium', 'low']

    title:        the text to display at the top of the matrix

    cmap:         the gradient of the values displayed from matplotlib.pyplot.cm
                  see http://matplotlib.org/examples/color/colormaps_reference.html
                  plt.get_cmap('jet') or plt.cm.Blues

    normalize:    If False, plot the raw numbers
                  If True, plot the proportions

    Usage
    -----
    plot_confusion_matrix(cm           = cm,                  # confusion matrix created by
                                                              # sklearn.metrics.confusion_matrix
                          normalize    = True,                # show proportions
                          target_names = y_labels_vals,       # list of names of the classes
                          title        = best_estimator_name) # title of graph

    Citiation
    ---------
    http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html

    """
    import matplotlib.pyplot as plt
    import numpy as np
    import itertools

    accuracy = np.trace(cm) / float(np.sum(cm))
    misclass = 1 - accuracy

    if cmap is None:
        cmap = plt.get_cmap('Blues')

    plt.figure(figsize=(8, 6))
    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()

    if target_names is not None:
        tick_marks = np.arange(len(target_names))
        plt.xticks(tick_marks, target_names, rotation=45)
        plt.yticks(tick_marks, target_names)

    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]


    thresh = cm.max() / 1.5 if normalize else cm.max() / 2
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        if normalize:
            plt.text(j, i, "{:0.4f}".format(cm[i, j]),
                     horizontalalignment="center",
                     color="white" if cm[i, j] > thresh else "black")
        else:
            plt.text(j, i, "{:,}".format(cm[i, j]),
                     horizontalalignment="center",
                     color="white" if cm[i, j] > thresh else "black")


    plt.tight_layout()
    plt.ylabel('True label')
    plt.xlabel('Predicted label\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))
    plt.show()

"""### 0. Linear Regression"""

reg = LinearRegression()
reg.fit(X_train, y_train)
y_pred = reg.predict(X_train)
y_pred = np.exp(y_pred)

print('Mean Absolute Error:', metrics.mean_absolute_error(np.exp(y_train), y_pred))  
print('Mean Squared Error:', metrics.mean_squared_error(np.exp(y_train), y_pred))  
print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(np.exp(y_train), y_pred)))
print('Variance score: %.2f' % metrics.r2_score(np.exp(y_train), y_pred))

y_pred

np.exp(y_train)

y_pred = reg.predict(X_test)

print('Mean Absolute Error:', metrics.mean_absolute_error(y_val, y_pred))  
print('Mean Squared Error:', metrics.mean_squared_error(y_val, y_pred))  
print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_val, y_pred)))
print('Variance score: %.2f' % metrics.r2_score(y_val, y_pred))

"""### 1. Logistic Regression"""

# Log reg

# Fit to training set
model = LogisticRegression(solver='saga', multi_class='multinomial', max_iter=3000)
model.fit(X_train,y_train)

# Predictions on training set
y_pred_train = model.predict(X_train)
metrics.confusion_matrix(y_train, y_pred_train, labels=[0, 1, 2, 3])
# pd.crosstab(y_train, y_pred, rownames=['True'], colnames=['Predicted'], margins=True)

metrics.f1_score(y_train, y_pred_train, average='micro')

y_train_b = label_binarize(y_train, bin_num)
y_pred_train_b = label_binarize(y_pred_train, bin_num)
roc_auc_score(y_train_b, y_pred_train_b)

# Predictions on val set
y_pred_val = model.predict(X_val)
metrics.confusion_matrix(y_val, y_pred_val, labels=[0, 1, 2, 3])

metrics.f1_score(y_val, y_pred_val, average='micro')

y_val_b = label_binarize(y_val, bin_num)
y_pred_val_b = label_binarize(y_pred_val, bin_num)
roc_auc_score(y_val_b, y_pred_val_b)

# Test set
y_pred_test = model.predict(X_test)
metrics.confusion_matrix(y_test, y_pred_test, labels=[0, 1, 2, 3])

metrics.f1_score(y_test, y_pred_test, average='micro')

y_test_b = label_binarize(y_test, bin_num)
y_pred_test_b = label_binarize(y_pred_test, bin_num)
roc_auc_score(y_test_b, y_pred_test_b)

# Plot normalized confusion matrix
plt.figure()
cm = metrics.confusion_matrix(y_test, y_pred_test, labels=[0, 1, 2, 3])
# cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
plot_confusion_matrix(cm, normalize=True, target_names=['0','1','2','3'],
                      title='Normalized confusion matrix')
plt.show()

#array = metrics.confusion_matrix(y_test, y_pred_test, labels=[0, 1, 2, 3])
#df_cm = pd.DataFrame(array, index = [i for i in "0123"],
#                  columns = [i for i in "0123"])
#plt.figure(figsize = (10,7))
#sn.heatmap(df_cm, annot=True)

# Learning Curve

# Learning curve
iter_num = np.linspace(start=1000,stop=2000,num=9)
f1_array_train = np.zeros(10)
f1_array_val = np.zeros(10)

for i in iter_num:
  model = LogisticRegression(solver='saga', multi_class='multinomial', max_iter=i)
  model.fit(X_train,y_train)

  # training set
  y_pred = model.predict(X_train)
  f1_array_train[i] = metrics.f1_score(y_train, y_pred, average='micro') 
  
  # val set
  y_val_pred = model.predict(X_val)
  f1_array_val = metrics.f1_score(y_train, y_val_pred, average='micro')

iter_num = np.linspace(start=1000,stop=2000,num=9)
iter_num

"""### 2. Neural Net"""

# Neural Net
scaler = StandardScaler()
scaler.fit(X_train)
X_train = scaler.transform(X_train)
X_val = scaler.transform(X_val)
mlp = MLPClassifier(hidden_layer_sizes=(100, 100, 100, 100),max_iter=800)
mlp.fit(X_train,y_train)

# training set prediction
y_pred_train = mlp.predict(X_train)
metrics.confusion_matrix(y_train, y_pred_train, labels=[0, 1, 2, 3])

metrics.f1_score(y_train, y_pred_train, average='micro')

y_train_b = label_binarize(y_train, bin_num)
y_pred_train_b = label_binarize(y_pred_train, bin_num)
roc_auc_score(y_train_b, y_pred_train_b)

# val set prediction
y_val_pred = mlp.predict(X_val)
metrics.confusion_matrix(y_val, y_val_pred, labels=[0, 1, 2, 3])

metrics.f1_score(y_val, y_val_pred, average='micro')

y_val_b = label_binarize(y_val, bin_num)
y_pred_val_b = label_binarize(y_val_pred, bin_num)
roc_auc_score(y_val_b, y_pred_val_b)

# Test set
y_pred_test = model.predict(X_test)
metrics.confusion_matrix(y_test, y_pred_test, labels=[0, 1, 2, 3])

metrics.f1_score(y_test, y_pred_test, average='micro')

y_test_b = label_binarize(y_test, bin_num)
y_pred_test_b = label_binarize(y_pred_test, bin_num)
roc_auc_score(y_test_b, y_pred_test_b)

# Plot normalized confusion matrix
plt.figure()
cm = metrics.confusion_matrix(y_test, y_pred_test, labels=[0, 1, 2, 3])
# cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
plot_confusion_matrix(cm, normalize=True, target_names=['0','1','2','3'],
                      title='Normalized confusion matrix')
plt.show()

# Learning curve
node_num = np.zeros(10)
f1_array_train = np.zeros(10)
f1_array_val = np.zeros(10)

for i in range(0,10):
  j = (i+1)*10
  mlp = MLPClassifier(hidden_layer_sizes=(j, j, j, j),max_iter=1000)
  mlp.fit(X_train,y_train)

  # training set
  y_pred = mlp.predict(X_train)
  f1_array_train[i] = metrics.f1_score(y_train, y_pred, average='micro') 
  
  # val set
  y_val_pred = mlp.predict(X_val)
  f1_array_val[i] = metrics.f1_score(y_val, y_val_pred, average='micro')

plt.plot([10, 20, 30, 40, 50, 60, 70, 80, 90, 100], f1_array_train, label='Training Set')
plt.plot([10, 20, 30, 40, 50, 60, 70, 80, 90, 100], f1_array_val, label='Test Set')
plt.xlabel('Nodes in Neural Network')
plt.ylabel('F1 Score')
plt.title('Learning Curve for Neural Network')
plt.legend()
plt.show()

"""### 3. Decision trees"""

# Decision trees
clf = tree.DecisionTreeClassifier()
clf = clf.fit(X_train, y_train)
clf.get_depth()

# training set prediction
y_pred_train = clf.predict(X_train)
metrics.confusion_matrix(y_train, y_pred_train, labels=[0, 1, 2, 3])

metrics.f1_score(y_train, y_pred_train, average='micro')

y_train_b = label_binarize(y_train, bin_num)
y_pred_train_b = label_binarize(y_pred_train, bin_num)
roc_auc_score(y_train_b, y_pred_train_b)

# val set prediction
y_pred_val = clf.predict(X_val)
metrics.confusion_matrix(y_val, y_pred_val, labels=[0, 1, 2, 3])

metrics.f1_score(y_val, y_pred_val, average='micro')

y_val_b = label_binarize(y_val, bin_num)
y_pred_val_b = label_binarize(y_pred_val, bin_num)
roc_auc_score(y_val_b, y_pred_val_b)

# Test set
y_pred_test = model.predict(X_test)
metrics.confusion_matrix(y_test, y_pred_test, labels=[0, 1, 2, 3])

metrics.f1_score(y_test, y_pred_test, average='micro')

y_test_b = label_binarize(y_test, bin_num)
y_pred_test_b = label_binarize(y_pred_test, bin_num)
roc_auc_score(y_test_b, y_pred_test_b)

# Plot normalized confusion matrix
plt.figure()
cm = metrics.confusion_matrix(y_test, y_pred_test, labels=[0, 1, 2, 3])
# cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
plot_confusion_matrix(cm, normalize=True, target_names=['0','1','2','3'],
                      title='Normalized confusion matrix')
plt.show()

# Learning Curve
depth_array = list(range(33))
f1_array_train = np.zeros(33)
f1_array_val = np.zeros(33)

for i in depth_array:
  clf = tree.DecisionTreeClassifier(max_depth=i+1)
  clf = clf.fit(X_train, y_train)

  # training set
  y_pred = clf.predict(X_train)
  f1_array_train[i] = metrics.f1_score(y_train, y_pred, average='micro') 
  
  # val set
  y_val_pred = clf.predict(X_val)
  f1_array_val[i] = metrics.f1_score(y_val, y_val_pred, average='micro') 

plt.plot(depth_array, f1_array_train, label='Training Set')
plt.plot(depth_array, f1_array_val, label='Test Set')
plt.xlabel('Depth of Tree')
plt.ylabel('F1 Score')
plt.title('Learning Curve for Decision Tree')
plt.legend()
plt.show()

"""### 4. Neural Net with Regression"""

mlp = MLPRegressor(hidden_layer_sizes=(100,100,100,100), max_iter=1000, shuffle=True, random_state=1)
mlp.fit(X_train,y_train)
mlp.score(X_train,y_train)

y_pred = mlp.predict(X_train)
y_pred = np.exp(y_pred)
y_train_true = np.exp(y_train)

y_pred

max_ind = np.argmax(y_train_true)
y_train_true[max_ind] - y_pred[max_ind]

print('Mean Absolute Error:', metrics.mean_absolute_error(y_train, y_pred))  
print('Mean Squared Error:', metrics.mean_squared_error(y_train, y_pred))  
print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_train, y_pred)))

mlp.score(X_val,y_val)

y_pred = mlp.predict(X_vak)
print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))  
print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))  
print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))

y_pred

y_val

